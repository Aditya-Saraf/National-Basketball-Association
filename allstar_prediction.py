# -*- coding: utf-8 -*-
"""Kaivan's Experiment on ALDA Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FltqCY6yFGX5sIS-QKk_GFizph7Oamc0
"""

from datetime import date, datetime
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
from matplotlib import pyplot
from sklearn.linear_model import LogisticRegression, SGDClassifier
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.tree import DecisionTreeClassifier

path = './data'

# taking dataframes
playoff_data_df = pd.read_csv(path + '/player_regular_season_career.csv')
regular_season_data_df = pd.read_csv(path + '/player_playoffs_career.csv')
biological_data_df = pd.read_csv(path + '/players.csv')

# Cleaning the data
playoff_data_df['ilkid'] = playoff_data_df['ilkid'].str.strip()
regular_season_data_df['ilkid'] = regular_season_data_df['ilkid'].str.strip()
biological_data_df['ilkid'] = biological_data_df['ilkid'].str.strip()

# Helper Methods
'''
Inputs: birthdate of a player.
Output: Age of a player as of now.
'''
def player_age(born):
  if born == '':
    return ''
  born = born.split(' ')[0].replace('-', '/')
  born = datetime.strptime(born, "%Y/%m/%d").date()
  today = date.today()
  return today.year - born.year - ((today.month, today.day) < (born.month, born.day))

'''
Inputs: First and last season of a player.
Output: Career span of a player.
'''
def start_age(df):
  birthdate, firstseason = df.birthdate, df.firstseason
  if birthdate == '' or firstseason == '':
    return ''
  birthdate = birthdate.split(' ')[0].replace('-', '/')
  birthdate = datetime.strptime(birthdate, "%Y/%m/%d").date()
  return int(firstseason - birthdate.year)

'''
Inputs: birthdate and last season of a player.
Output: Age of a player on his retirement.
'''
def retirement_age(df):
  birthdate, lastseason = df.birthdate, df.lastseason
  if birthdate == '' or lastseason == '':
    return ''
  birthdate = birthdate.split(' ')[0].replace('-', '/')
  birthdate = datetime.strptime(birthdate, "%Y/%m/%d").date()
  return int(lastseason - birthdate.year)

'''
Inputs: First and last season of a player.
Output: Career span of a player.
'''
def carrer_span(df):
  firstseason, lastseason = df.firstseason, df.lastseason
  if firstseason == '' or lastseason == '':
    return ''
  return lastseason - firstseason

'''
Inputs: Dataframe
Output: Renamed Dataframe
'''

def rename_df(df):
  old_keys = df.keys()
  rename_dict = {}
  for key in old_keys:
    if key[-2:] == '_x':
      new_key = 'rs_' + key[:-2]
      rename_dict[key] = new_key
    elif key[-2:] == '_y':
      new_key = 'po_' + key[:-2]
      rename_dict[key] = new_key
  df = df.rename(columns= rename_dict)
  return df



# joining Player'biological aspects and the career
career_df = pd.merge(playoff_data_df, regular_season_data_df, on='ilkid', how='outer')
player_df = pd.merge(career_df, biological_data_df, on='ilkid', how = 'left')

# Replacing null to empty string
player_df = player_df.fillna('')


# adding additional attribute to the data frame
player_df['age'] = pd.to_numeric(player_df.birthdate.apply(player_age), errors='coerce')
player_df.lastseason = pd.to_numeric(player_df.lastseason, errors='coerce', downcast='signed')
player_df['start_age'] = pd.to_numeric(player_df[['birthdate', 'firstseason']].apply(start_age, axis=1), errors='coerce')
player_df['retirement_age'] = pd.to_numeric(player_df[['birthdate', 'lastseason']].apply(retirement_age, axis=1), errors='coerce')
player_df['career_span'] = pd.to_numeric(player_df[['firstseason', 'lastseason']].apply(carrer_span, axis=1), errors='coerce')
player_df['height'] = player_df.h_feet * 12 + player_df.h_inches

#removing redudent attributes
player_df = player_df.drop(['firstname_y', 'lastname_y', 'firstname', 'lastname', 'birthdate', 'h_feet', 'h_inches']  , 1)

#rename attributes
player_df = rename_df(player_df)
player_df.to_csv(path + "/player_data.csv")
print(player_df.shape)
sns.heatmap(player_df.corr(), square=True);

# aggregation over allstar file
player_allstar_df = pd.read_csv(path + '/player_allstar.csv').groupby(['ilkid','leag'], as_index=False).agg(
    {"year":['min', 'max'],"gp":['sum'], "minutes":['sum'], "pts":['sum'],
     "dreb":['sum'] , "oreb":['sum'],"reb":['sum'],"asts":['sum'], "stl":['sum'], "blk":['sum'],
     "turnover":['sum'], "pf":['sum'], "fga":['sum'], "fgm":['sum'], "fta":['sum'],
     "tpa":['sum'], "tpm":['sum']})

player_allstar_career_df = pd.DataFrame()
player_allstar_career_df['ilkid'] = player_allstar_df['ilkid'].str.upper()
player_allstar_career_df['all_star_leag'] = player_allstar_df['leag']
player_allstar_career_df['all_star_firstyear'] = player_allstar_df['year']['min']
player_allstar_career_df['all_star_lastyear'] = player_allstar_df['year']['max']
player_allstar_career_df['all_star_gp'] = player_allstar_df['gp']['sum']
player_allstar_career_df['all_star_minutes'] = player_allstar_df['minutes']['sum']
player_allstar_career_df['all_star_total_pts'] = player_allstar_df['pts']['sum']
player_allstar_career_df['all_star_dreb'] = player_allstar_df['dreb']['sum']
player_allstar_career_df['all_star_oreb'] = player_allstar_df['oreb']['sum']
player_allstar_career_df['all_star_reb'] = player_allstar_df['reb']['sum']
player_allstar_career_df['all_star_asts'] = player_allstar_df['asts']['sum']
player_allstar_career_df['all_star_stl'] = player_allstar_df['stl']['sum']
player_allstar_career_df['all_star_blk'] = player_allstar_df['blk']['sum']
player_allstar_career_df['all_star_turnover'] = player_allstar_df['turnover']['sum']
player_allstar_career_df['all_star_pf'] = player_allstar_df['pf']['sum']
player_allstar_career_df['all_star_fga'] = player_allstar_df['fga']['sum']
player_allstar_career_df['all_star_fgm'] = player_allstar_df['fgm']['sum']
player_allstar_career_df['all_star_fta'] = player_allstar_df['fta']['sum']
player_allstar_career_df['all_star_tpa'] = player_allstar_df['tpa']['sum']
player_allstar_career_df['all_star_tpm'] = player_allstar_df['tpm']['sum']
player_allstar_career_df.to_csv(path + "/player_allstar_career.csv")
player_allstar_career_df.head()
print(player_allstar_career_df.shape)

"""Observation: 439 All star players among 3946 total players

# Experiment 1: odds of being in allstar
"""

all_star_ids = set(player_allstar_career_df['ilkid'].str.strip())

# introducing one field: Is_all_Star
for _, row in player_df.iterrows():
  if row.ilkid.strip() in all_star_ids:
    player_df.loc[player_df['ilkid'] == row.ilkid, 'is_allstar'] = 1
  else:
    player_df.loc[player_df['ilkid'] == row.ilkid, 'is_allstar'] = 0



X, y = player_df.drop(['ilkid', 'rs_firstname', 'rs_lastname', 'firstseason', 'lastseason', 'position', 'rs_leag', 'po_leag', 'college' ,'is_allstar'], axis=1), player_df['is_allstar']
X = X.fillna(0, axis=1)
X = X.replace('', 0)
X.head()
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

"""### Using Logistic regression"""


xax, yax1, yax2, yax3 = [], [], [], []
recall_count1, recall_count2 = 0, 0
allstar, notallstar = 0, 0
y_test = np.array(y_test)
min_iter, max_iter = 5, 500 
for i in range(min_iter, max_iter + 1):
  clf = make_pipeline(StandardScaler(), LogisticRegression(random_state=1, max_iter= i)).fit(X_train, y_train)
  y_pred = list(clf.predict(X_test))
  TN, FP, FN, TP = confusion_matrix(y_test, y_pred, labels=[0,1]).ravel()
  recall1 = (TP * 100) / (TP + FN)
  recall2 = (TN * 100) / (TN + FP)
  accuracy = ((TP + TN) * 100) / (TP + FP + TN + FN)
  yax1.append(recall1)
  yax2.append(recall2)
  yax3.append(accuracy)


plt.figure(figsize=(10,5))
print("Highest Recall for class 1: ", round(max(yax1), 2), '% on ', np.argmax(yax1), ' iterations')
plt.title("Recall vs number of iteration graph for class 1")
xax = range(min_iter, max_iter + 1)
plt.plot(xax, yax1, label = 'Class 1 Recall')
plt.plot(xax, yax2, label = 'Class 0 Recall')
plt.plot(xax, yax3, label = 'Accuracy')
plt.xlabel('Number of Iteration')
plt.ylabel('Recall')
plt.legend()
plt.show()

model = make_pipeline(StandardScaler(), LogisticRegression(random_state=1, max_iter= np.argmax(yax1) +5)).fit(X_train, y_train)
y_pred = list(model.predict(X_test))
TN, FP, FN, TP = confusion_matrix(y_test, y_pred, labels=[0,1]).ravel()

recall = TP*100/(TP+FN)
precision = TP*100/(TP+FP)
accuracy = (TP+TN)*100 / (TP+TN+FP+FN)
print("recall: ", recall, "%")
print("precision: ", precision, "%")
print("accuracy:", accuracy, "%")
LR_scores = [recall, precision, accuracy]

importance = np.abs(model.steps[1][1].coef_[0])
indices = np.argsort(importance)[::-1][:10]
# summarize feature importance
for i in indices:
  print('Feature: %0d, Score: %.5f' % (i,importance[i]))
# plot feature importance
pyplot.figure(figsize=(15,5))
pyplot.title("Top 10 important features")
pyplot.xlabel("Features")
pyplot.ylabel("Score")
pyplot.bar([X_test.keys()[i] for i in indices], [importance[i] for i in indices])
pyplot.show()

"""### Using Decision Tree"""



train_loss, test_loss = [], []
for i in range(2, 100):
  tree = make_pipeline(StandardScaler(), DecisionTreeClassifier(max_depth=20,
                                min_weight_fraction_leaf=0.0, max_features=None, 
                                random_state=None, max_leaf_nodes=i,
                                class_weight=None, ccp_alpha=0.0))

  # Train Decision Tree Classifer
  tree.fit(X_train,y_train)
  y_train_pred = tree.predict(X_train)
  misclassified_train = np.sum(np.abs(np.subtract(y_train_pred, y_train)))
  loss_train = (misclassified_train * 100 /len(y_train))
  train_loss.append(loss_train)

  y_pred = tree.predict(X_test)
  misclassified_test = np.sum(np.abs(np.subtract(y_pred, y_test)))
  loss_test = (misclassified_test * 100 /len(y_test))
  test_loss.append(loss_test)

plt.figure(figsize=(10,5))
xax = list(range(2, 100))

plt.title("Loss vs complexity of the model")
plt.plot(xax, train_loss, label = 'Training loss')
plt.plot(xax, test_loss, label = 'Testing loss')
plt.xlabel('model complexity')
plt.ylabel('loss')
plt.legend()
plt.show()
print("Best leaf nodes:", np.argmin(test_loss), "Achieving testing loss: ", min(test_loss))

tree = make_pipeline(StandardScaler(), DecisionTreeClassifier(max_depth=20,
                              min_weight_fraction_leaf=0.0, max_features=None, 
                              random_state=None, max_leaf_nodes=np.argmin(test_loss) + 2,
                              class_weight=None, ccp_alpha=0.0))
tree.fit(X_train,y_train)
y_pred = list(tree.predict(X_test))
TN, FP, FN, TP = confusion_matrix(y_test, y_pred, labels=[0,1]).ravel()

recall = TP*100/(TP+FN)
precision = TP*100/(TP+FP)
accuracy = (TP+TN)*100 / (TP+TN+FP+FN)
print("recall: ", recall, "%")
print("precision: ", precision, "%")
print("accuracy:", accuracy, "%")
DT_scores = [recall, precision, accuracy]

importance = np.abs(tree.steps[1][1].feature_importances_)
indices = np.argsort(importance)[::-1][:10]
# summarize feature importance
for i in indices:
  print('Feature: %0d, Score: %.5f' % (i,importance[i]))
# plot feature importance
pyplot.figure(figsize=(10,5))
pyplot.title("Top 10 important features")
pyplot.xlabel("Features")
pyplot.ylabel("Score")
pyplot.bar([X_test.keys()[i] for i in indices], [importance[i] for i in indices])
pyplot.show()

"""### Using KNN"""

train_loss, test_loss = [], []
K = list(range(1, 10))
for k in K:
  neigh = make_pipeline(StandardScaler(), KNeighborsClassifier(n_neighbors=k))
  neigh.fit(X_train, y_train)
  
  y_train_pred = neigh.predict(X_train)
  train_loss.append(np.sum(np.abs(np.subtract(y_train, y_train_pred))))
  y_test_pred = neigh.predict(X_test)
  test_loss.append(np.sum(np.abs(np.subtract(y_test, y_test_pred))))
print(K[np.argmin(test_loss)])
print(min(test_loss))

import matplotlib.pyplot as plt
import numpy as np

plt.figure(figsize=(10,5))
xax = K

plt.title("Loss vs K of the model")
plt.plot(xax, train_loss, label = 'Training loss')
plt.plot(xax, test_loss, label = 'Testing loss')
plt.xlabel('K')
plt.ylabel('loss')
plt.legend()
plt.show()
print("Best K :", K[np.argmin(test_loss)], "Achieving testing loss: ", min(test_loss))

best_k = K[np.argmin(test_loss)]
knn = make_pipeline(StandardScaler(), KNeighborsClassifier(n_neighbors=best_k))
knn.fit(X_train, y_train)
y_pred = list(knn.predict(X_test))
TN, FP, FN, TP = confusion_matrix(y_test, y_pred, labels=[0,1]).ravel()
recall = TP*100/(TP+FN)
precision = TP*100/(TP+FP)
accuracy = (TP+TN)*100 / (TP+TN+FP+FN)
print("recall: ", recall, "%")
print("precision: ", precision, "%")
print("accuracy:", accuracy, "%")
KNN_scores = [recall, precision, accuracy]

"""### Using SGD"""

sgd = make_pipeline(StandardScaler(), SGDClassifier(max_iter=90, tol=1e-3))
sgd.fit(X_train, y_train)
y_pred = list(sgd.predict(X_test))
TN, FP, FN, TP = confusion_matrix(y_test, y_pred, labels=[0,1]).ravel()

recall = TP*100/(TP+FN)
precision = TP*100/(TP+FP)
accuracy = (TP+TN)*100 / (TP+TN+FP+FN)
print("recall: ", recall, "%")
print("precision: ", precision, "%")
print("accuracy:", accuracy, "%")
SGD_scores = [recall, precision, accuracy]

"""## Comparing Algorithms"""

from functools import reduce

LR_scores = LR_scores[:3]
DT_scores = DT_scores[:3]
KNN_scores = KNN_scores[:3]
SGD_scores = SGD_scores[:3]

LR_scores.append((2 * LR_scores[0] * LR_scores[1]) / (LR_scores[0] + LR_scores[1]))
DT_scores.append((2 * DT_scores[0] * DT_scores[1]) / (DT_scores[0] + DT_scores[1]))
KNN_scores.append((2 * KNN_scores[0] * KNN_scores[1]) / (KNN_scores[0] + KNN_scores[1]))
SGD_scores.append((2 * SGD_scores[0] * SGD_scores[1]) / (SGD_scores[0] + SGD_scores[1]))

score_list = ["Recall", "Precision", "Accuracy", "F1-Score"]
logistic_regression_df = pd.DataFrame({"LR": LR_scores}, index = score_list)
decision_tree_df = pd.DataFrame({"DT": DT_scores}, index = score_list)
knn_df = pd.DataFrame({"KNN": KNN_scores}, index = score_list)
sgd_df = pd.DataFrame({"SGD": SGD_scores}, index = score_list)

model_scores = [logistic_regression_df, decision_tree_df, knn_df, sgd_df]
frame_scores = reduce(lambda x,y: pd.merge(x,y, left_index = True, right_index = True), model_scores).T
fig, ax  = plt.subplots(1,1, figsize = (10,5))
frame_scores.plot.bar(ax = ax, edgecolor = "black")
ax.legend(loc = 'best')
ax.set_ylabel("Score")
ax.set_title("Model comparision")

print(logistic_regression_df)
print("-" * 10)
print(decision_tree_df)
print("-" * 10)
print(knn_df)
print("-" * 10)
print(sgd_df)
plt.show()
